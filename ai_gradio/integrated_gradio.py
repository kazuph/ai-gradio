import os
import base64
import gradio as gr
import modelscope_studio.components.antd as antd
import modelscope_studio.components.base as ms
import re

# è¿½åŠ ï¼šOpenAI API ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã®importï¼ˆå¿…è¦ã«å¿œã˜ã¦ç’°å¢ƒå¤‰æ•°ãªã©ã§api_keyã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼‰
import openai
from openai import OpenAI

# æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«è¿½åŠ 
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from ai_gradio.logging_config import setup_logging

# ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–
logger = setup_logging()

# æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«è¿½åŠ 
from dotenv import load_dotenv
load_dotenv()  # è¿½åŠ : .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«èª­ã¿è¾¼ã¿ã¾ã™

# æ—¢å­˜ã®importã®ç›´å¾Œã«è¿½åŠ 
BASE_URL = os.environ.get("BASE_URL", "http://localhost:7860")

# å®šæ•°: å„provider:ãƒ¢ãƒ‡ãƒ«åã®ãƒªã‚¹ãƒˆ
INTEGRATED_MODELS = [
    "openai:o3-mini",
    "openai:o3-mini-high",
    "openai:gpt-4o-mini", 
    "openai:gpt-4o", 
    "anthropic:claude-3-5-sonnet-20241022", 
    "gemini:gemini-2.0-flash",
    "gemini:gemini-2.0-flash-lite-preview-02-05",
    "gemini:gemini-2.0-pro-exp-02-05",
    "gemini:gemini-2.0-flash-thinking-exp-01-21",
    # "gemini:gemini-exp-1206",
    # "gemini:gemini-1.5-pro", 
    # "deepseek:deepseek-r1",
]

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆWebã‚¢ãƒ—ãƒªç”Ÿæˆç”¨ï¼‰
DEFAULT_WEBAPP_SYSTEM_PROMPT = """You are an expert web developer. When asked to create a web application:
1. Always respond with HTML code wrapped in ```html code blocks.
2. Include necessary CSS within <style> tags.
3. Include necessary JavaScript within <script> tags.
4. Ensure the code is complete and self-contained.
5. Add helpful comments explaining key parts of the code.
6. Focus on creating a functional and visually appealing result.
7. Additionally, an internal LLM API is available at POST /api/llm.
   - To use this API, send a JSON object with a 'prompt' field containing your textual prompt.
   - The server will relay your request using the default gemini-2.0-flash model and respond with plain text.
   - When calling the API from client-side JavaScript, use a complete URL (e.g., `/api/llm`) or access the application via http://localhost:7860 so that the relative URL is properly resolved.
   - Ensure you include proper error handling when invoking this API."""

# é€šå¸¸ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
DEFAULT_TEXT_SYSTEM_PROMPT = """Before coding, make a plan inside a <thinking> tag.
1. Identify core requirement
2. Consider 3 implementation approaches
3. Choose simplest that meets needs
4. Verify with these questions:
   - Can this be split into smaller functions?
   - Are there unnecessary abstractions?
   - Will this be clear to a junior dev?

For example:
<thinking>
Let me think through this step by step.
...
</thinking>

You are a helpful assistant. Provide concise and informative answers to user queries."""

# å„provideræ¯ã®LLMç”Ÿæˆé–¢æ•°ã®å®Ÿè£…ã‚’æ›´æ–°
def generate_openai(query, model, system_prompt, prompt_type):
    try:
        logger.info(f"Starting OpenAI generation with model {model}")
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set.")
        
        client = OpenAI(api_key=api_key)
        
        # system_prompt ã‚’ä½¿ç”¨ (å¼•æ•°ã¨ã—ã¦å—ã‘å–ã‚‹)
        
        # ãƒ¢ãƒ‡ãƒ«åã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‡¦ç†
        if model in ("openai:o3-mini-high", "o3-mini-high"):
            actual_model = "o3-mini"
        else:
            actual_model = model.replace("openai:", "")
        
        # prompt_type ã«å¿œã˜ã¦ user ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®šã™ã‚‹
        if prompt_type == "Web App":
            user_msg = f"Create a web application that: {query}"
        else:
            user_msg = query
        
        # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§å…±é€šï¼‰
        params = {
            "model": actual_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_msg}
            ],
            "stream": False
        }
        
        # o3-mini-highã®å ´åˆã¯reasoning_effortã‚’è¨­å®š
        if model in ("openai:o3-mini-high", "o3-mini-high"):
            params["reasoning_effort"] = "high"
        # o3ç³»ä»¥å¤–ã®ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        elif not actual_model.startswith("o3-"):
            params.update({
                "max_tokens": 2048,
                "temperature": 0.7
            })
        
        response = client.chat.completions.create(**params)
        
        response_text = response.choices[0].message.content
        code = remove_code_block(response_text)
        preview = send_to_preview(code)
        logger.info(f"Successfully completed OpenAI generation with {model} ({actual_model})")
        return code, preview
    except Exception as e:
        logger.error(f"Error in OpenAI generation: {str(e)}")
        err = f"Error in OpenAI: {str(e)}"
        return err, f"<div style='padding: 8px;color:red;'>{err}</div>"

def generate_anthropic(query, model, system_prompt, prompt_type):
    try:
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY environment variable is not set.")
        
        from anthropic import Anthropic
        client = Anthropic(api_key=api_key)
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ”¹å–„ (å¼•æ•°ã‚’ä½¿ç”¨)
        
        if prompt_type == "Web App":
            content = f"{system_prompt}\n\nCreate a web application that: {query}"
        else:
            content = f"{system_prompt}\n\n{query}"
        response = client.messages.create(
            model=model,
            max_tokens=2048,
            messages=[{
                "role": "user",
                "content": content
            }]
        )
        
        response_text = response.content[0].text
        code = remove_code_block(response_text)
        preview = send_to_preview(code)
        return code, preview
    except Exception as e:
        err = f"Error in Anthropic: {str(e)}"
        return err, f"<div style='padding: 8px;color:red;'>{err}</div>"

def generate_gemini(query, model, system_prompt, prompt_type):
    try:
        load_dotenv()
        
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable is not set.")
        
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        
        model = genai.GenerativeModel(model_name=model)
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (å¼•æ•°ã‚’ä½¿ç”¨)
        
        if prompt_type == "Web App":
            last_message = f"Create a web application that: {query}"
        else:
            last_message = query
        response = model.generate_content(
            [
                {"role": "user", "parts": [{"text": system_prompt}]},
                {"role": "model", "parts": [{"text": "I understand and will follow these guidelines."}]},
                {"role": "user", "parts": [{"text": last_message}]}
            ],
            stream=False
        )
        
        response_text = response.text
        code = remove_code_block(response_text)
        preview = send_to_preview(code)
        return code, preview
    except Exception as e:
        err = f"Error in Gemini: {str(e)}"
        return err, f"<div style='padding: 8px;color:red;'>{err}</div>"

def generate_deepseek(query, model, system_prompt, prompt_type):
    try:
        # DeepSeek APIã‚­ãƒ¼ã®å–å¾—
        api_key = os.environ.get("DEEPSEEK_API_KEY")
        if not api_key:
            raise ValueError("DEEPSEEK_API_KEY environment variable is not set.")
        
        client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com/v1"
        )
        
        # DeepSeek API å‘¼ã³å‡ºã— (system_promptã‚’ä½¿ç”¨)
        if prompt_type == "Web App":
            user_msg = f"Create a web application that: {query}"
        else:
            user_msg = query
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_msg}
            ],
            temperature=0.7,
            max_tokens=2048,
            stream=False
        )
        
        response_text = response.choices[0].message.content
        code = remove_code_block(response_text)
        preview = send_to_preview(code)
        return code, preview
    except Exception as e:
        err = f"Error in DeepSeek: {str(e)}"
        return err, f"<div style='padding: 8px;color:red;'>{err}</div>"

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã®è¿½åŠ 
def remove_code_block(text):
    """ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹"""
    pattern = r'```(?:html)?\n(.+?)\n```'
    match = re.search(pattern, text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text.strip()

# send_to_previewé–¢æ•°ã‚’æ›´æ–°
def send_to_preview(code, iframe_id=""):
    """
    HTMLãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ã§ã™ã€‚
    ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã« <base> ã‚¿ã‚°ã‚’è¿½åŠ ã—ã€iframe å†…ã§ã®ç›¸å¯¾ URL ã®è§£æ±ºã‚’ä¿è¨¼ã—ã¾ã™ã€‚
    """
    clean_code = code.replace("```html", "").replace("```", "").strip()
    
    # æ—¢ã«HTMLæ–‡æ›¸ã§ãªã‘ã‚Œã°ã€<base>ã‚¿ã‚°ä»˜ãã®HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ãƒ©ãƒƒãƒ—
    if "<html" not in clean_code.lower():
        wrapped_code = f"""<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <base href="{BASE_URL}/">
  </head>
  <body>
    {clean_code}
  </body>
</html>"""
    else:
        # æ—¢å­˜ã®HTMLãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆã¯<head>ã‚¿ã‚°å†…ã«<base>ã‚¿ã‚°ã‚’è¿½åŠ 
        if "<head>" in clean_code.lower():
            wrapped_code = clean_code.replace(
                "<head>",
                f'<head>\n    <base href="{BASE_URL}/">'
            )
        else:
            # <head>ã‚¿ã‚°ãŒãªã„å ´åˆã¯è¿½åŠ 
            wrapped_code = clean_code.replace(
                "<html>",
                f'<html>\n  <head>\n    <base href="{BASE_URL}/">\n  </head>'
            )

    encoded_html = base64.b64encode(wrapped_code.encode('utf-8')).decode('utf-8')
    data_uri = f"data:text/html;charset=utf-8;base64,{encoded_html}"
    
    id_attribute = f' id="{iframe_id}"' if iframe_id else ""
    return f'''
        <iframe{id_attribute}
            src="{data_uri}"
            style="width:100%;border:none;border-radius:4px;"
            sandbox="allow-scripts allow-same-origin"
        ></iframe>
    '''

# æ—¢å­˜ã®importã«è¿½åŠ 
import asyncio
from concurrent.futures import ThreadPoolExecutor

# éåŒæœŸã®LLMç”Ÿæˆé–¢æ•°ã‚’æ›´æ–°
async def async_generate_openai(query, model, system_prompt, prompt_type):
    try:
        # åŒæœŸçš„ãª generate_openai ã‚’ asyncio.to_thread ã§å®Ÿè¡Œã—ã€ä¸¦åˆ—åŒ–
        return await asyncio.to_thread(generate_openai, query, model, system_prompt, prompt_type)
    except Exception as e:
        logger.error(f"Error in async OpenAI generation: {str(e)}")
        return (f"Error in OpenAI: {str(e)}",
                f"<div style='padding: 8px;color:red;'>Error in OpenAI: {str(e)}</div>")

async def async_generate_anthropic(query, model, system_prompt, prompt_type):
    try:
        # åŒæœŸçš„ãª generate_anthropic ã‚’ asyncio.to_thread ã§å®Ÿè¡Œã—ã€ä¸¦åˆ—åŒ–
        return await asyncio.to_thread(generate_anthropic, query, model, system_prompt, prompt_type)
    except Exception as e:
        logger.error(f"Error in async Anthropic generation: {str(e)}")
        return (f"Error in Anthropic: {str(e)}",
                f"<div style='padding: 8px;color:red;'>Error in Anthropic: {str(e)}</div>")

async def async_generate_gemini(query, model, system_prompt, prompt_type):
    try:
        # åŒæœŸçš„ãª generate_gemini ã‚’ asyncio.to_thread ã§å®Ÿè¡Œã—ã€ä¸¦åˆ—åŒ–
        return await asyncio.to_thread(generate_gemini, query, model, system_prompt, prompt_type)
    except Exception as e:
        logger.error(f"Error in async Gemini generation: {str(e)}")
        return (f"Error in Gemini: {str(e)}",
                f"<div style='padding: 8px;color:red;'>Error in Gemini: {str(e)}</div>")

async def async_generate_deepseek(query, model, system_prompt, prompt_type):
    try:
        # åŒæœŸçš„ãª generate_deepseek ã‚’ asyncio.to_thread ã§å®Ÿè¡Œã—ã€ä¸¦åˆ—åŒ–
        return await asyncio.to_thread(generate_deepseek, query, model, system_prompt, prompt_type)
    except Exception as e:
        logger.error(f"Error in async DeepSeek generation: {str(e)}")
        return (f"Error in DeepSeek: {str(e)}",
                f"<div style='padding: 8px;color:red;'>Error in DeepSeek: {str(e)}</div>")

# çµ±åˆç”Ÿæˆé–¢æ•°ã‚’ç°¡ç´ åŒ–
async def generate_parallel(query, selected_models, system_prompt, prompt_type):
    logger.info(f"Received generation request - Query: {query}")
    logger.info(f"Selected models: {selected_models}")
    
    # åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶å¾¡ã™ã‚‹ã‚»ãƒãƒ•ã‚©ã‚’ä½œæˆï¼ˆå¿…è¦ã«å¿œã˜ã¦æ•°å€¤ã‚’èª¿æ•´ï¼‰
    semaphore = asyncio.Semaphore(5)  # åŒæ™‚ã«5ã¤ã¾ã§å®Ÿè¡Œå¯èƒ½
    
    async def run_with_semaphore(full_model, task):
        async with semaphore:
            return await task
    
    tasks = []
    for full_model in selected_models:
        try:
            provider, model = full_model.split(":")
            logger.info(f"Preparing task for {full_model}")
            
            if provider == "openai":
                task = async_generate_openai(query, model, system_prompt, prompt_type)
            elif provider == "anthropic":
                task = async_generate_anthropic(query, model, system_prompt, prompt_type)
            elif provider == "gemini":
                task = async_generate_gemini(query, model, system_prompt, prompt_type)
            elif provider == "deepseek":
                task = async_generate_deepseek(query, model, system_prompt, prompt_type)
            else:
                logger.error(f"Unknown provider: {full_model}")
                continue
            
            tasks.append((full_model, run_with_semaphore(full_model, task)))
            
        except Exception as e:
            logger.error(f"Error preparing task for {full_model}: {str(e)}")
            continue
    
    results = []
    if tasks:
        completed_tasks = await asyncio.gather(*(task for _, task in tasks))
        for (full_model, _), result in zip(tasks, completed_tasks):
            # await ã§çµæœã‚’å–ã‚Šå‡ºã—ã¦ã‹ã‚‰ãƒªã‚¹ãƒˆã«è¿½åŠ 
            code, preview = result
            results.append((full_model, code, preview))

    # HTMLã®ç”Ÿæˆ
    grid_html = """
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-markup.min.js"></script>
    <style>
        :root {
            --card-bg: #ffffff;
            --border-color: #e0e0e0;
            --header-bg: #f5f5f5;
            --code-bg: #1e1e1e;
            --text-color: #333333;
            --preview-bg: #ffffff;
            --preview-border: #e0e0e0;
            --button-bg: rgba(0, 0, 0, 0.1);
            --button-hover: rgba(0, 0, 0, 0.2);
            --button-color: #333333;
            /* ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆã‚¹ã‚¿ãƒƒã‚¯ã‚’ä½¿ç”¨ */
            --system-fonts: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, 
                          Oxygen-Sans, Ubuntu, Cantarell, "Helvetica Neue", sans-serif;
        }

        /* å…¨ä½“ã®ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š */
        * {
            font-family: var(--system-fonts);
        }

        .results-container {
            width: 100%;
            padding: 20px;
            overflow-x: auto;
            background-color: var(--card-bg);
            color: var(--text-color);
            font-family: var(--system-fonts);
        }

        .results-grid {
            display: flex;
            flex-wrap: wrap;
            gap: 24px;
            justify-content: center;
        }

        .result-card {
            width: 800px;
            min-width: 800px;
            background: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            overflow: hidden;
            margin-bottom: 24px;
        }

        .card-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 16px;
            background: var(--header-bg);
            border-bottom: 1px solid var(--border-color);
            color: var(--text-color);
        }

        .preview-container {
            width: 100%;
            aspect-ratio: 1;
            position: relative;
            background: var(--preview-bg);
            border: 1px solid var(--preview-border);
            border-radius: 4px;
            margin: 16px;
        }

        .preview-container iframe {
            width: 100%;
            height: 100%;
            border: none;
            background: var(--preview-bg);
        }

        .code-content {
            display: none;
            padding: 16px;
            background: var(--code-bg);
            max-height: 400px;
            overflow-y: auto;
            margin: 16px;
            border-radius: 4px;
        }

        .code-content pre {
            margin: 0;
        }

        .code-content code {
            color: #e0e0e0;
        }

        .header-buttons {
            display: flex;
            gap: 8px;
        }

        .button-icon {
            width: 32px;
            height: 32px;
            padding: 6px;
            border-radius: 6px;
            border: 1px solid var(--border-color);
            cursor: pointer;
            background: var(--button-bg);
            color: var(--button-color);
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.2s ease;
        }

        .button-icon:hover {
            background: var(--button-hover);
        }

        .button-icon svg {
            width: 18px;
            height: 18px;
            fill: currentColor;
        }

        @media (max-width: 768px) {
            .results-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
    <div class='results-container'>
        <div class='results-grid'>
    """

    # çµæœã‚«ãƒ¼ãƒ‰ã®ç”Ÿæˆ
    for full_model, code, preview in results:
        provider, model_name = full_model.split(":")
        model_id = f"model_{provider}_{model_name}".replace("-", "_")
        
        preview_iframe = send_to_preview(code, iframe_id=f"{model_id}_preview")
        escaped_code = code.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
        
        grid_html += f"""
            <div class='result-card'>
                <div class='card-header'>
                    <div class='header-title'>
                        <strong>{provider.upper()}</strong> - {model_name}
                    </div>
                    <div class='header-buttons'>
                        <button class="button-icon" onclick="(function(){{ 
                            var codeEl = document.getElementById('{model_id}_code'); 
                            if (codeEl){{ 
                                codeEl.style.display = (codeEl.style.display === 'none' ? 'block' : 'none'); 
                                if (codeEl.style.display === 'block' && window.Prism){{{{ Prism.highlightAll(); }}}} 
                            }} 
                        }})()" title="ã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¤º/éè¡¨ç¤º">
                            <svg viewBox="0 0 24 24">
                                <path fill="currentColor" d="M9.4 16.6L4.8 12l4.6-4.6L8 6l-6 6 6 6 1.4-1.4zm5.2 0l4.6-4.6-4.6-4.6L16 6l6 6-6 6-1.4-1.4z"/>
                            </svg>
                        </button>
                        <button class="button-icon" onclick="(function(){{ 
                            var iframe = document.getElementById('{model_id}_preview');
                            if (iframe && iframe.contentWindow){{ 
                                iframe.contentWindow.location.reload();
                            }} 
                        }})()" title="ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ›´æ–°">
                            <svg viewBox="0 0 24 24">
                                <path fill="currentColor" d="M17.65 6.35A7.958 7.958 0 0012 4c-4.42 0-7.99 3.58-7.99 8s3.57 8 7.99 8c3.73 0 6.84-2.55 7.73-6h-2.08A5.99 5.99 0 0112 18c-3.31 0-6-2.69-6-6s2.69-6 6-6c1.66 0 3.14.69 4.22 1.78L13 11h7V4l-2.35 2.35z"/>
                            </svg>
                        </button>
                    </div>
                </div>
                <div style='position: relative;'>
                    <div class='preview-container'>
                        {preview_iframe}
                    </div>
                    <div id='{model_id}_code' class='code-content' style='display:none;'>
                        <pre><code class="language-html">{escaped_code}</code></pre>
                    </div>
                </div>
            </div>
        """

    grid_html += """
        </div>
    </div>
    """
    
    logger.info("Completed generating HTML grid")
    return grid_html

# çµ±åˆGradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®å®šç¾©
def build_interface():
    # CSSã§ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆã‚’å…¨ä½“ã«é©ç”¨ã™ã‚‹
    custom_css = """
    * {
        font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
    }
    """
    with gr.Blocks(css=custom_css) as demo:
        gr.Markdown("# ğŸ¨ AI Gradio Code Generator")
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("## å…¥åŠ›")
                # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚¨ãƒªã‚¢
                query_input = gr.Textbox(
                    placeholder="ä½œæˆã—ãŸã„Webã‚¢ãƒ—ãƒªã®ä»•æ§˜ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„",
                    label="Request",
                    lines=3
                )
                # ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆ: çµ±åˆå¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
                model_select = gr.Dropdown(
                    choices=INTEGRATED_MODELS,
                    value=[
                        INTEGRATED_MODELS[0], 
                        INTEGRATED_MODELS[4],
                        INTEGRATED_MODELS[7],
                    ],
                    multiselect=True,
                    label="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
                    info="è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã¾ã™"
                )

                # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³
                prompt_type = gr.Radio(
                    ["Web App", "Text"],  # é¸æŠè‚¢
                    label="Prompt Type",
                    value="Web App",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ "Web App" ã«è¨­å®š
                    interactive=True
                )

                # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›æ¬„ (Web App ç”¨, Textç”¨)
                system_prompt_webapp_textbox = gr.Textbox(
                    placeholder="Enter system prompt for web app generation...",
                    label="System Prompt (Web App)",
                    lines=5,
                    value=DEFAULT_WEBAPP_SYSTEM_PROMPT,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Web Appç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®š
                    visible=True
                )
                system_prompt_text_textbox = gr.Textbox(
                    placeholder="Enter system prompt for text generation...",
                    label="System Prompt (Text)",
                    lines=5,
                    value=DEFAULT_TEXT_SYSTEM_PROMPT,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®š
                    visible=False  # æœ€åˆã¯éè¡¨ç¤º
                )
                
                # system_prompt_webapp_textbox ãŒå¤‰æ›´ã•ã‚ŒãŸã¨ãã« prompt_type ã‚‚ "Web App" ã«è¨­å®š
                system_prompt_webapp_textbox.change(lambda: "Web App", inputs=[], outputs=[prompt_type])
                # system_prompt_text_textbox ãŒå¤‰æ›´ã•ã‚ŒãŸã¨ãã« prompt_type ã‚‚ "Text" ã«è¨­å®š
                system_prompt_text_textbox.change(lambda: "Text", inputs=[], outputs=[prompt_type])

                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦è¡¨ç¤ºã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹é–¢æ•°
                def switch_prompt_visibility(prompt_type):
                    if prompt_type == "Web App":
                        return gr.update(visible=True), gr.update(visible=False)  # Web App ç”¨ã‚’è¡¨ç¤ºã€Textç”¨ã‚’éè¡¨ç¤º
                    else:
                        return gr.update(visible=False), gr.update(visible=True) # Web App ç”¨ã‚’éè¡¨ç¤ºã€Textç”¨ã‚’è¡¨ç¤º

                # ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ãŒå¤‰æ›´ã•ã‚ŒãŸã¨ãã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
                prompt_type.change(
                    switch_prompt_visibility,
                    inputs=[prompt_type],
                    outputs=[system_prompt_webapp_textbox, system_prompt_text_textbox])

                generate_btn = gr.Button(
                    "Generate",
                    variant="primary",
                    size="lg"
                )
            with gr.Column(scale=2):
                gr.Markdown("## çµæœ")
                # çµæœå‡ºåŠ›ç”¨ã®HTMLã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
                output_html = gr.HTML(
                    container=True,
                    show_label=True
                )

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ system_prompt ã‚’æ±ºå®šã™ã‚‹é–¢æ•°
        def get_system_prompt(prompt_type, webapp_prompt, text_prompt):
            if prompt_type == "Web App":
                return webapp_prompt  # ç·¨é›†ã•ã‚ŒãŸã€ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Web Appç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            else:
                return text_prompt  # ç·¨é›†ã•ã‚ŒãŸã€ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Textç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

        # ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯æ™‚ã®å‡¦ç†
        async def run_generate(q, m, pt, wp, tp):
            # get_system_promptã§ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ã—ã€prompt_type(pt)ã‚‚æ¸¡ã™
            return await generate_parallel(q, m, get_system_prompt(pt, wp, tp), pt)

        generate_btn.click(
            fn=run_generate,
            inputs=[query_input, model_select, prompt_type, system_prompt_webapp_textbox, system_prompt_text_textbox],
            outputs=output_html
        )
    return demo

if __name__ == "__main__":
    demo = build_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        show_error=True
    ) 