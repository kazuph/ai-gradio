import os
import base64
import gradio as gr
import modelscope_studio.components.antd as antd
import modelscope_studio.components.base as ms
import re

# è¿½åŠ ï¼šOpenAI API ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã®importï¼ˆå¿…è¦ã«å¿œã˜ã¦ç’°å¢ƒå¤‰æ•°ãªã©ã§api_keyã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼‰
import openai
from openai import OpenAI

# æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«è¿½åŠ 
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from ai_gradio.logging_config import setup_logging

# ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–
logger = setup_logging()
import asyncio
generation_lock = asyncio.Lock()  # ç”Ÿæˆå‡¦ç†ã®é‡è¤‡å®Ÿè¡Œã‚’é˜²ããŸã‚ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ­ãƒƒã‚¯

# æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«è¿½åŠ 
from dotenv import load_dotenv
load_dotenv()  # è¿½åŠ : .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«èª­ã¿è¾¼ã¿ã¾ã™

# æ—¢å­˜ã®importã®ç›´å¾Œã«è¿½åŠ 
BASE_URL = os.environ.get("BASE_URL", "http://localhost:7860")

# å®šæ•°: å„provider:ãƒ¢ãƒ‡ãƒ«åã®ãƒªã‚¹ãƒˆ
INTEGRATED_MODELS = [
    "openai:o3-mini",
    "openai:o3-mini-high",
    "openai:gpt-4o-mini",
    "openai:gpt-4o",
    "openai:chatgpt-4o-latest",
    "anthropic:claude-3-5-sonnet-20241022",
    "anthropic:claude-3-7-sonnet-20250219",
    "gemini:gemini-2.0-pro-exp-02-05",
    "gemini:gemini-2.0-flash",
    "gemini:gemini-2.0-flash-lite-preview-02-05",
    "gemini:gemini-2.0-flash-thinking-exp-01-21",
    # "gemini:gemini-exp-1206",
    # "gemini:gemini-1.5-pro",
    # "deepseek:deepseek-r1",
]

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆWebã‚¢ãƒ—ãƒªç”Ÿæˆç”¨ï¼‰ã‚’æ›´æ–°
DEFAULT_WEBAPP_SYSTEM_PROMPT = """You are an expert web developer. When asked to create a web application:
1. Always respond with HTML code wrapped in ```html code blocks.
2. Include necessary CSS within <style> tags.
3. Include necessary JavaScript within <script> tags.
4. Ensure the code is complete and self-contained.
5. Add helpful comments explaining key parts of the code.
6. Focus on creating a functional and visually appealing result.
7. Additionally, an internal LLM API is available at POST /api/llm.
   - To use this API, send a JSON object with:
     * 'prompt' field containing your textual prompt
     * 'format_type' field set to either "text" or "json"
   - Example request for JSON response:
     fetch('/api/llm', {
       method: 'POST',
       headers: { 'Content-Type': 'application/json' },
       body: JSON.stringify({
         prompt: 'Convert 42 to Roman numerals and return as JSON',
         format_type: 'json'
       })
     })
   - When format_type is "json", ensure your prompt asks for JSON format.
   - Example JSON response format:
     {
       "number": 42,
       "roman": "XLII"
     }
   - Note: Even with format_type="json", the response might be wrapped in ```json code blocks.
     The API will automatically handle this and extract the JSON content.
   - For text responses, omit format_type or set it to "text"
   - The default model is gemini-2.0-flash
   - Ensure you include proper error handling when invoking this API."""

# é€šå¸¸ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
DEFAULT_TEXT_SYSTEM_PROMPT = """Before coding, make a plan inside a <thinking> tag.
1. Identify core requirement
2. Consider 3 implementation approaches
3. Choose simplest that meets needs
4. Verify with these questions:
   - Can this be split into smaller functions?
   - Are there unnecessary abstractions?
   - Will this be clear to a junior dev?

For example:
<thinking>
Let me think through this step by step.
...
</thinking>

You are a helpful assistant. Provide concise and informative answers to user queries."""

# å„provideræ¯ã®LLMç”Ÿæˆé–¢æ•°ã®å®Ÿè£…ã‚’æ›´æ–°
def generate_openai(query, model, system_prompt, prompt_type):
    try:
        logger.info(f"Starting OpenAI generation with model {model}")
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set.")

        client = OpenAI(api_key=api_key)

        # system_prompt ã‚’ä½¿ç”¨ (å¼•æ•°ã¨ã—ã¦å—ã‘å–ã‚‹)

        # ãƒ¢ãƒ‡ãƒ«åã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‡¦ç†
        if model in ("openai:o3-mini-high", "o3-mini-high"):
            actual_model = "o3-mini"
        else:
            actual_model = model.replace("openai:", "")

        # prompt_type ã«å¿œã˜ã¦ user ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®šã™ã‚‹
        if prompt_type == "Web App":
            user_msg = f"Create a web application that: {query}"
        else:
            user_msg = query

        # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§å…±é€šï¼‰
        params = {
            "model": actual_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_msg}
            ],
            "stream": False
        }

        # o3-mini-highã®å ´åˆã¯reasoning_effortã‚’è¨­å®š
        if model in ("openai:o3-mini-high", "o3-mini-high"):
            params["reasoning_effort"] = "high"
        # o3ç³»ä»¥å¤–ã®ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        elif not actual_model.startswith("o3-"):
            params.update({
                "max_tokens": 2048,
                "temperature": 0.7
            })

        response = client.chat.completions.create(**params)

        response_text = response.choices[0].message.content
        code = remove_code_block(response_text)
        preview = send_to_preview(code)
        logger.info(f"Successfully completed OpenAI generation with {model} ({actual_model})")
        return code, preview
    except Exception as e:
        logger.error(f"Error in OpenAI generation: {str(e)}")
        err = f"Error in OpenAI: {str(e)}"
        return err, f"<div style='padding: 8px;color:red;'>{err}</div>"

def generate_anthropic(query, model, system_prompt, prompt_type):
    try:
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY environment variable is not set.")

        from anthropic import Anthropic
        client = Anthropic(api_key=api_key)

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ”¹å–„ (å¼•æ•°ã‚’ä½¿ç”¨)

        if prompt_type == "Web App":
            content = f"{system_prompt}\n\nCreate a web application that: {query}"
        else:
            content = f"{system_prompt}\n\n{query}"
        response = client.messages.create(
            model=model,
            max_tokens=2048,
            messages=[{
                "role": "user",
                "content": content
            }]
        )

        response_text = response.content[0].text
        code = remove_code_block(response_text)
        preview = send_to_preview(code)
        return code, preview
    except Exception as e:
        err = f"Error in Anthropic: {str(e)}"
        return err, f"<div style='padding: 8px;color:red;'>{err}</div>"

def generate_gemini(query, model, system_prompt, prompt_type):
    try:
        load_dotenv()

        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable is not set.")

        import google.generativeai as genai
        genai.configure(api_key=api_key)

        model = genai.GenerativeModel(model_name=model)

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (å¼•æ•°ã‚’ä½¿ç”¨)

        if prompt_type == "Web App":
            last_message = f"Create a web application that: {query}"
        else:
            last_message = query
        response = model.generate_content(
            [
                {"role": "user", "parts": [{"text": system_prompt}]},
                {"role": "model", "parts": [{"text": "I understand and will follow these guidelines."}]},
                {"role": "user", "parts": [{"text": last_message}]}
            ],
            stream=False
        )

        response_text = response.text
        code = remove_code_block(response_text)
        preview = send_to_preview(code)
        return code, preview
    except Exception as e:
        err = f"Error in Gemini: {str(e)}"
        return err, f"<div style='padding: 8px;color:red;'>{err}</div>"

def generate_deepseek(query, model, system_prompt, prompt_type):
    try:
        # DeepSeek APIã‚­ãƒ¼ã®å–å¾—
        api_key = os.environ.get("DEEPSEEK_API_KEY")
        if not api_key:
            raise ValueError("DEEPSEEK_API_KEY environment variable is not set.")

        client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com/v1"
        )

        # DeepSeek API å‘¼ã³å‡ºã— (system_promptã‚’ä½¿ç”¨)
        if prompt_type == "Web App":
            user_msg = f"Create a web application that: {query}"
        else:
            user_msg = query
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_msg}
            ],
            temperature=0.7,
            max_tokens=2048,
            stream=False
        )

        response_text = response.choices[0].message.content
        code = remove_code_block(response_text)
        preview = send_to_preview(code)
        return code, preview
    except Exception as e:
        err = f"Error in DeepSeek: {str(e)}"
        return err, f"<div style='padding: 8px;color:red;'>{err}</div>"

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã®è¿½åŠ 
def remove_code_block(text):
    """ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹"""
    pattern = r'```(?:html)?\n(.+?)\n```'
    match = re.search(pattern, text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text.strip()

# send_to_previewé–¢æ•°ã‚’æ›´æ–°
def send_to_preview(code, iframe_id=""):
    """
    HTMLãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ã§ã™ã€‚
    ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã« <base> ã‚¿ã‚°ã‚’è¿½åŠ ã—ã€iframe å†…ã§ã®ç›¸å¯¾ URL ã®è§£æ±ºã‚’ä¿è¨¼ã—ã¾ã™ã€‚
    """
    clean_code = code.replace("```html", "").replace("```", "").strip()

    # æ—¢ã«HTMLæ–‡æ›¸ã§ãªã‘ã‚Œã°ã€<base>ã‚¿ã‚°ä»˜ãã®HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ãƒ©ãƒƒãƒ—
    if "<html" not in clean_code.lower():
        wrapped_code = f"""<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <base href="{BASE_URL}/">
  </head>
  <body>
    {clean_code}
  </body>
</html>"""
    else:
        # æ—¢å­˜ã®HTMLãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆã¯<head>ã‚¿ã‚°å†…ã«<base>ã‚¿ã‚°ã‚’è¿½åŠ 
        if "<head>" in clean_code.lower():
            wrapped_code = clean_code.replace(
                "<head>",
                f'<head>\n    <base href="{BASE_URL}/">'
            )
        else:
            # <head>ã‚¿ã‚°ãŒãªã„å ´åˆã¯è¿½åŠ 
            wrapped_code = clean_code.replace(
                "<html>",
                f'<html>\n  <head>\n    <base href="{BASE_URL}/">\n  </head>'
            )

    encoded_html = base64.b64encode(wrapped_code.encode('utf-8')).decode('utf-8')
    data_uri = f"data:text/html;charset=utf-8;base64,{encoded_html}"

    id_attribute = f' id="{iframe_id}"' if iframe_id else ""
    return f'''
        <iframe{id_attribute}
            src="{data_uri}"
            style="width:100%;border:none;border-radius:4px;"
            sandbox="allow-scripts allow-same-origin"
        ></iframe>
    '''

# æ—¢å­˜ã®send_to_previewé–¢æ•°ã¯ãã®ã¾ã¾ã¨ã—ã¦ã€ä»¥ä¸‹ã«æ–°ã—ã„é–¢æ•° send_to_preview_react ã‚’è¿½åŠ 

def send_to_preview_react(react_code, container_id=""):
    """
    LLM ãŒç”Ÿæˆã—ãŸ React ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ã§ã™ã€‚

    â€» ã“ã®å®Ÿè£…ã¯è©¦ä½œç”¨ã§ã‚ã‚Šã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ã¯æœ€å°é™ã§ã™ã€‚

    ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã¯ã€Reactã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆä¾‹: GeneratedComponentï¼‰ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹å‰æã§ã™ã€‚
    Babel ã‚’åˆ©ç”¨ã—ã¦ JSX ã‚’ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã€ReactDOM ã§ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚
    """
    if not container_id:
        container_id = "react_preview"
    html_react = f"""
    <div id="{container_id}"></div>
    <!-- React ã¨ ReactDOM ã®èª­ã¿è¾¼ã¿ï¼ˆé–‹ç™ºç”¨ç‰ˆï¼‰ -->
    <script crossorigin src="https://unpkg.com/react@17/umd/react.development.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@17/umd/react-dom.development.js"></script>
    <!-- Babel ã®èª­ã¿è¾¼ã¿ï¼ˆJSXã‚’ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹ãŸã‚ï¼‰ -->
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <!-- ç”Ÿæˆã•ã‚ŒãŸ React ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€ã‚¹ã‚¯ãƒªãƒ—ãƒˆ -->
    <script type="text/babel">
    {react_code}
    // ä¾‹: LLM ã«ã‚ˆã‚Šç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰å†…ã§ GeneratedComponent ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã¨ä»®å®š
    ReactDOM.render(<GeneratedComponent />, document.getElementById("{container_id}"));
    </script>
    """
    return html_react

# æ—¢å­˜ã®importã«è¿½åŠ 
from concurrent.futures import ThreadPoolExecutor

# éåŒæœŸã®LLMç”Ÿæˆé–¢æ•°ã‚’æ›´æ–°
async def async_generate_openai(query, model, system_prompt, prompt_type):
    try:
        # åŒæœŸçš„ãª generate_openai ã‚’ asyncio.to_thread ã§å®Ÿè¡Œã—ã€ä¸¦åˆ—åŒ–
        return await asyncio.to_thread(generate_openai, query, model, system_prompt, prompt_type)
    except Exception as e:
        logger.error(f"Error in async OpenAI generation: {str(e)}")
        return (f"Error in OpenAI: {str(e)}",
                f"<div style='padding: 8px;color:red;'>Error in OpenAI: {str(e)}</div>")

async def async_generate_anthropic(query, model, system_prompt, prompt_type):
    try:
        # åŒæœŸçš„ãª generate_anthropic ã‚’ asyncio.to_thread ã§å®Ÿè¡Œã—ã€ä¸¦åˆ—åŒ–
        return await asyncio.to_thread(generate_anthropic, query, model, system_prompt, prompt_type)
    except Exception as e:
        logger.error(f"Error in async Anthropic generation: {str(e)}")
        return (f"Error in Anthropic: {str(e)}",
                f"<div style='padding: 8px;color:red;'>Error in Anthropic: {str(e)}</div>")

async def async_generate_gemini(query, model, system_prompt, prompt_type):
    try:
        # åŒæœŸçš„ãª generate_gemini ã‚’ asyncio.to_thread ã§å®Ÿè¡Œã—ã€ä¸¦åˆ—åŒ–
        return await asyncio.to_thread(generate_gemini, query, model, system_prompt, prompt_type)
    except Exception as e:
        logger.error(f"Error in async Gemini generation: {str(e)}")
        return (f"Error in Gemini: {str(e)}",
                f"<div style='padding: 8px;color:red;'>Error in Gemini: {str(e)}</div>")

async def async_generate_deepseek(query, model, system_prompt, prompt_type):
    try:
        # åŒæœŸçš„ãª generate_deepseek ã‚’ asyncio.to_thread ã§å®Ÿè¡Œã—ã€ä¸¦åˆ—åŒ–
        return await asyncio.to_thread(generate_deepseek, query, model, system_prompt, prompt_type)
    except Exception as e:
        logger.error(f"Error in async DeepSeek generation: {str(e)}")
        return (f"Error in DeepSeek: {str(e)}",
                f"<div style='padding: 8px;color:red;'>Error in DeepSeek: {str(e)}</div>")

# çµ±åˆç”Ÿæˆé–¢æ•°ã‚’ç°¡ç´ åŒ–
async def get_implementation_plan(query, prompt_type):
    """o3-miniã‚’ä½¿ç”¨ã—ã¦å®Ÿè£…è¨ˆç”»ã‚’ç”Ÿæˆã™ã‚‹"""
    try:
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set.")

        client = OpenAI(api_key=api_key)

        planning_prompt = """ã‚ãªãŸã¯å„ªç§€ãªã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®è¦ä»¶ã«å¯¾ã™ã‚‹å®Ÿè£…è¨ˆç”»ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

1. è¦ä»¶ã®åˆ†æ
2. å¿…è¦ãªæ©Ÿèƒ½ã®æ´—ã„å‡ºã—
3. å®Ÿè£…æ‰‹é †ã®è©³ç´°åŒ–
4. æ³¨æ„ç‚¹ã‚„ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

å›ç­”ã¯ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¡Œã£ã¦ãã ã•ã„ï¼š

<å®Ÿè£…è¨ˆç”»>
[ã“ã“ã«è¨ˆç”»ã®è©³ç´°ã‚’è¨˜è¼‰]
</å®Ÿè£…è¨ˆç”»>"""

        if prompt_type == "Web App":
            user_msg = f"ä»¥ä¸‹ã®Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè£…è¨ˆç”»ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š{query}"
        else:
            user_msg = f"ä»¥ä¸‹ã®æ©Ÿèƒ½ã®å®Ÿè£…è¨ˆç”»ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š{query}"

        response = client.chat.completions.create(
            model="o3-mini",
            messages=[
                {"role": "system", "content": planning_prompt},
                {"role": "user", "content": user_msg}
            ],
            stream=False
        )

        plan = response.choices[0].message.content
        return plan
    except Exception as e:
        logger.error(f"Error in implementation planning: {str(e)}")
        return f"Error in planning: {str(e)}"

async def generate_parallel(query, selected_models, system_prompt, prompt_type, use_planning=False):
    logger.info(f"Received generation request - Query: {query}")
    logger.info(f"Selected models: {selected_models}")

    implementation_plan = ""
    if use_planning:
        implementation_plan = await get_implementation_plan(query, prompt_type)
        # å®Ÿè£…è¨ˆç”»ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
        system_prompt = f"{system_prompt}\n\nå®Ÿè£…è¨ˆç”»ï¼š\n{implementation_plan}"

    # åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶å¾¡ã™ã‚‹ã‚»ãƒãƒ•ã‚©ã‚’ä½œæˆï¼ˆå¿…è¦ã«å¿œã˜ã¦æ•°å€¤ã‚’èª¿æ•´ï¼‰
    semaphore = asyncio.Semaphore(5)  # åŒæ™‚ã«5ã¤ã¾ã§å®Ÿè¡Œå¯èƒ½

    async def run_with_semaphore(full_model, task):
        async with semaphore:
            return await task

    tasks = []
    for full_model in selected_models:
        try:
            provider, model = full_model.split(":")
            logger.info(f"Preparing task for {full_model}")

            if provider == "openai":
                task = async_generate_openai(query, model, system_prompt, prompt_type)
            elif provider == "anthropic":
                task = async_generate_anthropic(query, model, system_prompt, prompt_type)
            elif provider == "gemini":
                task = async_generate_gemini(query, model, system_prompt, prompt_type)
            elif provider == "deepseek":
                task = async_generate_deepseek(query, model, system_prompt, prompt_type)
            else:
                logger.error(f"Unknown provider: {full_model}")
                continue

            tasks.append((full_model, run_with_semaphore(full_model, task)))

        except Exception as e:
            logger.error(f"Error preparing task for {full_model}: {str(e)}")
            continue

    results = []
    if tasks:
        completed_tasks = await asyncio.gather(*(task for _, task in tasks))
        for (full_model, _), result in zip(tasks, completed_tasks):
            # await ã§çµæœã‚’å–ã‚Šå‡ºã—ã¦ã‹ã‚‰ãƒªã‚¹ãƒˆã«è¿½åŠ 
            code, preview = result
            results.append((full_model, code, preview))

    # HTMLã®ç”Ÿæˆï¼ˆplan_htmlã‚’å‰Šé™¤ï¼‰
    grid_html = """
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-coy.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-markup.min.js"></script>
    <div class='results-container'>
        <div class='results-grid'>
    """

    # çµæœã‚«ãƒ¼ãƒ‰ã®ç”Ÿæˆ
    for full_model, code, preview in results:
        provider, model_name = full_model.split(":")
        model_id = f"model_{provider}_{model_name}".replace("-", "_")

        preview_iframe = send_to_preview(code, iframe_id=f"{model_id}_preview")
        escaped_code = code.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

        grid_html += f"""
            <div class='result-card'>
                <div class='card-header'>
                    <div class='header-title'>
                        <strong>{provider.upper()}</strong> - {model_name}
                    </div>
                    <div class='header-buttons'>
                        <button class="button-icon" onclick="(function(){{
                            var codeEl = document.getElementById('{model_id}_code');
                            if (codeEl){{
                                codeEl.style.display = (codeEl.style.display === 'none' ? 'block' : 'none');
                                if (codeEl.style.display === 'block' && window.Prism){{ Prism.highlightAll(); }}
                            }}
                        }})()" title="ã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¤º/éè¡¨ç¤º">
                            <svg viewBox="0 0 24 24">
                                <path fill="currentColor" d="M9.4 16.6L4.8 12l4.6-4.6L8 6l-6 6 6 6 1.4-1.4zm5.2 0l4.6-4.6-4.6-4.6L16 6l6 6-6 6-1.4-1.4z"/>
                            </svg>
                        </button>
                        <button class="button-icon" onclick="(function(){{ 
                            var iframe = document.getElementById('{model_id}_preview');
                            if (iframe){{
                                var currentSrc = iframe.src;
                                iframe.src = 'about:blank';
                                setTimeout(function() {{
                                    iframe.src = currentSrc;
                                }}, 100);
                            }}
                        }})()" title="ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ›´æ–°">
                            <svg viewBox="0 0 24 24">
                                <path fill="currentColor" d="M17.65 6.35A7.958 7.958 0 0012 4c-4.42 0-7.99 3.58-7.99 8s3.57 8 7.99 8c3.73 0 6.84-2.55 7.73-6h-2.08A5.99 5.99 0 0112 18c-3.31 0-6-2.69-6-6s2.69-6 6-6c1.66 0 3.14.69 4.22 1.78L13 11h7V4l-2.35 2.35z"/>
                            </svg>
                        </button>
                    </div>
                </div>
                <div style='position: relative;'>
                    <div class='preview-container'>
                        {preview_iframe}
                    </div>
                    <div id='{model_id}_code' class='code-content' style='display:none;'>
                        <pre><code class="language-html">{escaped_code}</code></pre>
                    </div>
                </div>
            </div>
        """

    grid_html += """
        </div>
    </div>
    """

    logger.info("Completed generating HTML grid")
    return grid_html

# çµ±åˆGradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®å®šç¾©
def build_interface():
    custom_css = """
    :root {
        --card-bg: #ffffff;
        --header-bg: #f5f5f5;
        --border-color: #e0e0e0;
        --text-color: #333333;
        --icon-color: #333333;
        --button-hover-bg: rgba(0, 0, 0, 0.1);
        --code-bg: #f5f5f5;
        --code-text: #333333;
        --preview-bg: #ffffff;
        --preview-border: #e0e0e0;
    }
    * {
        font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
    }

    /* çµæœå‡ºåŠ›ã‚¨ãƒªã‚¢ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .result-output {
        min-height: 600px !important;
        margin-bottom: 2rem;
    }

    /* ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ã‚³ãƒ³ãƒ†ãƒŠ */
    .progress-container {
        background: var(--neutral-100);
        padding: 1rem;
        border-radius: 8px;
        min-height: 100px;
        display: flex;
        align-items: center;
        justify-content: center;
    }

    /* ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .progress-bar {
        height: 4px;
        background: var(--primary-500);
        border-radius: 2px;
        animation: progress 2s infinite;
    }

    @keyframes progress {
        0% { width: 0%; }
        50% { width: 70%; }
        100% { width: 100%; }
    }

    /* å®Ÿè£…è¨ˆç”»ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .implementation-plan {
        margin: 20px 0;
        padding: 20px;
        border-radius: 8px;
        background: var(--neutral-50);
    }

    /* çµæœã‚«ãƒ¼ãƒ‰ã®ã‚¹ã‚¿ã‚¤ãƒ«æ›´æ–° */
    .result-card {
        background: var(--card-bg);
        border: 1px solid var(--border-color);
        border-radius: 8px;
        overflow: hidden;
        margin-bottom: 32px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        color: var(--text-color);
    }

    /* ã‚«ãƒ¼ãƒ‰ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ«ã®ä¿®æ­£ */
    .card-header {
        background: var(--header-bg, #f5f5f5);
        border-bottom: 1px solid var(--border-color, #e0e0e0);
        color: var(--text-color, #333333);
    }

    /* ãƒ˜ãƒƒãƒ€ãƒ¼å†…ã®å¼·èª¿ãƒ†ã‚­ã‚¹ãƒˆã®ä¿®æ­£ */
    .header-title strong {
        color: var(--text-color, #333333);
        margin-right: 4px;
    }

    /* ãƒ˜ãƒƒãƒ€ãƒ¼ãƒœã‚¿ãƒ³ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .header-buttons {
        display: flex;
        gap: 8px;
    }

    .button-icon {
        background: none;
        border: 1px solid var(--border-color, #404040);
        cursor: pointer;
        padding: 4px;
        border-radius: 4px;
        display: flex;
        align-items: center;
        justify-content: center;
        width: 32px;
        height: 32px;
        transition: all 0.2s;
        color: var(--icon-color, #333333);
    }

    .button-icon:hover {
        background-color: var(--button-hover-bg, rgba(0, 0, 0, 0.1));
        border-color: var(--icon-color, #333333);
    }

    .button-icon svg {
        width: 20px;
        height: 20px;
        color: var(--icon-color, #333333);
    }

    .preview-container {
        width: 100%;
        min-height: 600px !important;  /* æœ€å°é«˜ã•ã‚’è¨­å®š */
        height: 800px;  /* ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é«˜ã•ã‚’è¨­å®š */
        position: relative;
        background: var(--preview-bg);
        border: 1px solid var(--preview-border);
        border-radius: 4px;
        margin: 16px;
        overflow: auto;  /* ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ã« */
    }

    .preview-container iframe {
        width: 100%;
        height: 100%;
        min-height: 600px !important;  /* iframeã®æœ€å°é«˜ã•ã‚‚è¨­å®š */
        border: none;
        background: var(--preview-bg);
    }

    /* ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .code-content {
        background: var(--code-bg, #1e1e1e);
        color: var(--code-text, #d4d4d4);
        padding: 16px;
        margin: 16px;
        border-radius: 4px;
        overflow-x: auto;
        max-height: 800px;
        overflow-y: auto;
        border: 1px solid var(--border-color);
    }

    .code-content pre {
        margin: 0;
        padding: 0;
        background: transparent;
    }

    .code-content code {
        font-family: 'Consolas', 'Monaco', 'Andale Mono', monospace;
        font-size: 14px;
        line-height: 1.5;
        color: inherit;
    }
    """
    with gr.Blocks(
        css=custom_css,
        theme=gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="slate",
            neutral_hue="slate",
            font=["system-ui", "-apple-system", "sans-serif"]
        ).set(
            background_fill_primary="#ffffff",
            background_fill_secondary="#f5f5f5",
            border_color_primary="#e0e0e0"
        )
    ) as demo:
        gr.Markdown("# ğŸ¨ AI Gradio Code Generator")

        # å…¥åŠ›ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        gr.Markdown("## å…¥åŠ›")
        with gr.Row():
            # å·¦å´ã®ã‚«ãƒ©ãƒ 
            with gr.Column(scale=1):
                # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚¨ãƒªã‚¢
                query_input = gr.Textbox(
                    placeholder="ä½œæˆã—ãŸã„Webã‚¢ãƒ—ãƒªã®ä»•æ§˜ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„",
                    label="Request",
                    lines=8
                )
                # ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆ: çµ±åˆå¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
                model_select = gr.Dropdown(
                    choices=INTEGRATED_MODELS,
                    value=[
                       # INTEGRATED_MODELS[4],
                        # INTEGRATED_MODELS[5],
                        # INTEGRATED_MODELS[6],
                        INTEGRATED_MODELS[7],
                        # INTEGRATED_MODELS[8],
                    ],
                    multiselect=True,
                    label="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
                    info="è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã¾ã™"
                )

                # å®Ÿè£…è¨ˆç”»ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ã“ã“ã«ç§»å‹•
                use_planning = gr.Radio(
                    choices=["ã¯ã„", "ã„ã„ãˆ"],
                    label="o3-miniã«ã‚ˆã‚‹å®Ÿè£…è¨ˆç”»ã‚’åˆ©ç”¨ã—ã¾ã™ã‹ï¼Ÿ",
                    value="ã„ã„ãˆ",
                    info="o3-miniãŒå®Ÿè£…è¨ˆç”»ã‚’ä½œæˆã—ã€ãã®è¨ˆç”»ã«åŸºã¥ã„ã¦å„ãƒ¢ãƒ‡ãƒ«ãŒå®Ÿè£…ã‚’è¡Œã„ã¾ã™ã€‚"
                )

            # å³å´ã®ã‚«ãƒ©ãƒ 
            with gr.Column(scale=1):
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³
                prompt_type = gr.Radio(
                    ["Web App", "Text"],
                    label="Prompt Type",
                    value="Web App",
                    interactive=True
                )

                # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›æ¬„ (Web App ç”¨, Textç”¨)
                system_prompt_webapp_textbox = gr.Textbox(
                    placeholder="Enter system prompt for web app generation...",
                    label="System Prompt (Web App)",
                    lines=5,
                    value=DEFAULT_WEBAPP_SYSTEM_PROMPT,
                    visible=True
                )
                system_prompt_text_textbox = gr.Textbox(
                    placeholder="Enter system prompt for text generation...",
                    label="System Prompt (Text)",
                    lines=5,
                    value=DEFAULT_TEXT_SYSTEM_PROMPT,
                    visible=False
                )

        # å®Ÿè£…è¨ˆç”»ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆçµæœã®å‰ã«é…ç½®ï¼‰
        plan_output = gr.Markdown(
            visible=False,
            elem_classes="implementation-plan"
        )

        # Generate ãƒœã‚¿ãƒ³
        generate_btn = gr.Button(
            "Generate",
            variant="primary",
            size="lg"
        )

        # çµæœã‚»ã‚¯ã‚·ãƒ§ãƒ³
        gr.Markdown("## çµæœ")
        output_html = gr.HTML(
            container=True,
            show_label=True,
            elem_classes="result-output"
        )

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ system_prompt ã‚’æ±ºå®šã™ã‚‹é–¢æ•°
        def get_system_prompt(prompt_type, webapp_prompt, text_prompt):
            if prompt_type == "Web App":
                return webapp_prompt  # ç·¨é›†ã•ã‚ŒãŸã€ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Web Appç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            else:
                return text_prompt  # ç·¨é›†ã•ã‚ŒãŸã€ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Textç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

        # ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯æ™‚ã®å‡¦ç†ã‚’æ›´æ–°
        async def run_generate(q, m, pt, wp, tp, up):
            try:
                # éãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ã§ã®ãƒ­ãƒƒã‚¯å–å¾—ã‚’è©¦ã¿ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’çŸ­ãè¨­å®šï¼‰
                await asyncio.wait_for(generation_lock.acquire(), timeout=0.001)
            except asyncio.TimeoutError:
                logger.info("Generation is already in progress. Ignoring duplicate request.")
                return [plan_output, "<div style='padding: 8px;color:red;'>Process already in progress. Please wait...</div>"]
            try:
                use_plan = (up == "ã¯ã„")
                if use_plan:
                    implementation_plan = await get_implementation_plan(q, pt)
                    logger.info("Implementation Plan:")
                    logger.info(implementation_plan)
                    plan_output.visible = True
                    plan_output.value = f"## å®Ÿè£…è¨ˆç”» (o3-mini)\n\n{implementation_plan}"
                else:
                    plan_output.visible = False
                    implementation_plan = ""

                result = await generate_parallel(
                    q, m,
                    get_system_prompt(pt, wp, tp),
                    pt,
                    use_planning=use_plan
                )

                return [plan_output, result]
            finally:
                generation_lock.release()


        generate_btn.click(
            fn=run_generate,
            inputs=[
                query_input,
                model_select,
                prompt_type,
                system_prompt_webapp_textbox,
                system_prompt_text_textbox,
                use_planning
            ],
            outputs=[plan_output, output_html]
        )
    return demo

if __name__ == "__main__":
    demo = build_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        show_error=True
    ) 
