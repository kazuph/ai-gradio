import os
import base64
import gradio as gr
import modelscope_studio.components.antd as antd
import modelscope_studio.components.base as ms
import re
import requests
import zlib

# è¿½åŠ ï¼šOpenAI API ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã®importï¼ˆå¿…è¦ã«å¿œã˜ã¦ç’°å¢ƒå¤‰æ•°ãªã©ã§api_keyã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼‰
import openai
from openai import OpenAI

# æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«è¿½åŠ 
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from ai_gradio.logging_config import setup_logging

# ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–
logger = setup_logging()
import asyncio
generation_lock = asyncio.Lock()  # ç”Ÿæˆå‡¦ç†ã®é‡è¤‡å®Ÿè¡Œã‚’é˜²ããŸã‚ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ­ãƒƒã‚¯

# æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«è¿½åŠ 
from dotenv import load_dotenv
load_dotenv()  # è¿½åŠ : .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«èª­ã¿è¾¼ã¿ã¾ã™

# æ—¢å­˜ã®importã®ç›´å¾Œã«è¿½åŠ 
BASE_URL = os.environ.get("BASE_URL", "http://localhost:7860")

# å®šæ•°: å„provider:ãƒ¢ãƒ‡ãƒ«åã®ãƒªã‚¹ãƒˆ
INTEGRATED_MODELS = [
    "openai:o3-mini",
    "openai:o3-mini-high",
    "openai:gpt-4o-mini",
    "openai:gpt-4o",
    "openai:chatgpt-4o-latest",
    "anthropic:claude-3-5-sonnet-20241022",
    "anthropic:claude-3-7-sonnet-20250219",
    "anthropic:claude-3-7-sonnet-20250219-thinking",
    "gemini:gemini-2.0-pro-exp-02-05",
    "gemini:gemini-2.0-flash",
    "gemini:gemini-2.0-flash-lite-preview-02-05",
    "gemini:gemini-2.0-flash-thinking-exp-01-21",
    # "gemini:gemini-exp-1206",
    # "gemini:gemini-1.5-pro",
    # "deepseek:deepseek-r1",
]

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆWebã‚¢ãƒ—ãƒªç”Ÿæˆç”¨ï¼‰ã‚’æ›´æ–°
DEFAULT_WEBAPP_SYSTEM_PROMPT = """You are an expert web developer. When asked to create a web application:
1. Always respond with HTML code wrapped in ```html code blocks.
2. Include necessary CSS within <style> tags.
3. Include necessary JavaScript within <script> tags.
4. Ensure the code is complete and self-contained.
5. Add helpful comments explaining key parts of the code.
6. Focus on creating a functional and visually appealing result.
7. Additionally, an internal LLM API is available at POST /api/llm.
   - To use this API, send a JSON object with:
     * 'prompt' field containing your textual prompt
     * 'format_type' field set to either "text" or "json"
   - Example request for JSON response:
     fetch('/api/llm', {
       method: 'POST',
       headers: { 'Content-Type': 'application/json' },
       body: JSON.stringify({
         prompt: 'Convert 42 to Roman numerals and return as JSON',
         format_type: 'json'
       })
     })
   - When format_type is "json", ensure your prompt asks for JSON format.
   - Example JSON response format:
     {
       "number": 42,
       "roman": "XLII"
     }
   - Note: Even with format_type="json", the response might be wrapped in ```json code blocks.
     The API will automatically handle this and extract the JSON content.
   - For text responses, omit format_type or set it to "text"
   - The default model is gemini-2.0-flash
   - Ensure you include proper error handling when invoking this API."""

# é€šå¸¸ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
DEFAULT_TEXT_SYSTEM_PROMPT = """Before coding, make a plan inside a <thinking> tag.
1. Identify core requirement
2. Consider 3 implementation approaches
3. Choose simplest that meets needs
4. Verify with these questions:
   - Can this be split into smaller functions?
   - Are there unnecessary abstractions?
   - Will this be clear to a junior dev?

For example:
<thinking>
Let me think through this step by step.
...
</thinking>

You are a helpful assistant. Provide concise and informative answers to user queries."""

# Excalidrawå›³ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
DEFAULT_EXCALIDRAW_SYSTEM_PROMPT = """You are an expert diagram creator using Excalidraw format. When asked to create a diagram:
1. Always respond with ONLY valid Excalidraw JSON format wrapped in ```json code blocks.
2. Follow the Excalidraw JSON schema with required fields: type, version, source, elements.
3. Each element should have appropriate properties like type, x, y, width, height, etc.
4. Do not include any explanations or text outside the JSON code block.
5. Focus on creating clear, visually effective diagrams.
6. The diagram will be rendered using kroki.io's Excalidraw renderer.

Example of valid Excalidraw JSON format:
```json
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "type": "rectangle",
      "version": 175,
      "versionNonce": 279344008,
      "isDeleted": false,
      "id": "2ZYh24ed28FJ0yE-S3YNY",
      "fillStyle": "hachure",
      "strokeWidth": 1,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "angle": 0,
      "x": 580,
      "y": 140,
      "strokeColor": "#000000",
      "backgroundColor": "#15aabf",
      "width": 80,
      "height": 20,
      "seed": 521916552,
      "groupIds": [],
      "strokeSharpness": "sharp",
      "boundElementIds": []
    }
  ]
}
```"""

# GraphVizå›³ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
DEFAULT_GRAPHVIZ_SYSTEM_PROMPT = """You are an expert diagram creator using GraphViz DOT language. When asked to create a diagram:
1. Always respond with ONLY valid GraphViz DOT code wrapped in ```graphviz code blocks.
2. Use appropriate node shapes, colors, and edge styles to create clear visualizations.
3. Do not include any explanations or text outside the code block.
4. Focus on creating clear, visually effective diagrams.
5. The diagram will be rendered using kroki.io's GraphViz renderer.

Example of valid GraphViz DOT code:
```graphviz
digraph G {
  rankdir=LR;
  node [shape=box, style=filled, fillcolor=lightblue];
  
  A [label="Start"];
  B [label="Process"];
  C [label="Decision", shape=diamond, fillcolor=lightyellow];
  D [label="End"];
  
  A -> B;
  B -> C;
  C -> D [label="Yes"];
  C -> B [label="No"];
}
```"""

# Mermaidå›³ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
DEFAULT_MERMAID_SYSTEM_PROMPT = """You are an expert diagram creator using Mermaid syntax. When asked to create a diagram:
1. Always respond with ONLY valid Mermaid code wrapped in ```mermaid code blocks.
2. Use appropriate Mermaid diagram types: flowchart, sequence, class, state, entity-relationship, gantt, pie, etc.
3. Do not include any explanations or text outside the code block.
4. Focus on creating clear, visually effective diagrams.
5. The diagram will be rendered using kroki.io's Mermaid renderer.

Example of valid Mermaid code:
```mermaid
graph TD
    A[Start] --> B{Is it?}
    B -->|Yes| C[OK]
    C --> D[Rethink]
    D --> B
    B ---->|No| E[End]
```

Or for a sequence diagram:
```mermaid
sequenceDiagram
    participant Alice
    participant Bob
    Alice->>John: Hello John, how are you?
    loop Healthcheck
        John->>John: Fight against hypochondria
    end
    Note right of John: Rational thoughts <br/>prevail!
    John-->>Alice: Great!
    John->>Bob: How about you?
    Bob-->>John: Jolly good!
```"""

# å„provideræ¯ã®LLMç”Ÿæˆé–¢æ•°ã®å®Ÿè£…ã‚’æ›´æ–°
def generate_openai(query, model, system_prompt, prompt_type):
    try:
        logger.info(f"Starting OpenAI generation with model {model}")
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set.")

        client = OpenAI(api_key=api_key)

        # system_prompt ã‚’ä½¿ç”¨ (å¼•æ•°ã¨ã—ã¦å—ã‘å–ã‚‹)

        # ãƒ¢ãƒ‡ãƒ«åã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‡¦ç†
        if model in ("openai:o3-mini-high", "o3-mini-high"):
            actual_model = "o3-mini"
        else:
            actual_model = model.replace("openai:", "")

        # prompt_type ã«å¿œã˜ã¦ user ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®šã™ã‚‹
        if prompt_type == "Web App":
            user_msg = f"Create a web application that: {query}"
        else:
            user_msg = query

        # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§å…±é€šï¼‰
        params = {
            "model": actual_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_msg}
            ],
            "stream": False
        }

        # o3-mini-highã®å ´åˆã¯reasoning_effortã‚’è¨­å®š
        if model in ("openai:o3-mini-high", "o3-mini-high"):
            params["reasoning_effort"] = "high"
        # o3ç³»ä»¥å¤–ã®ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        elif not actual_model.startswith("o3-"):
            params.update({
                "max_tokens": 2048,
                "temperature": 0.7
            })

        response = client.chat.completions.create(**params)

        response_text = response.choices[0].message.content
        code = remove_code_block(response_text)
        preview = send_to_preview(code)
        logger.info(f"Successfully completed OpenAI generation with {model} ({actual_model})")
        return code, preview
    except Exception as e:
        logger.error(f"Error in OpenAI generation: {str(e)}")
        err = f"Error in OpenAI: {str(e)}"
        return err, f"<div style='padding: 8px;color:red;'>{err}</div>"

def generate_anthropic(query, model, system_prompt, prompt_type):
    try:
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY environment variable is not set.")

        from anthropic import Anthropic
        client = Anthropic(api_key=api_key)

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ”¹å–„ (å¼•æ•°ã‚’ä½¿ç”¨)

        if prompt_type == "Web App":
            content = f"{system_prompt}\n\nCreate a web application that: {query}"
        else:
            content = f"{system_prompt}\n\n{query}"
        response = client.messages.create(
            model=model,
            max_tokens=2048,
            messages=[{
                "role": "user",
                "content": content
            }]
        )

        response_text = response.content[0].text
        code = remove_code_block(response_text)
        preview = send_to_preview(code)
        return code, preview
    except Exception as e:
        err = f"Error in Anthropic: {str(e)}"
        return err, f"<div style='padding: 8px;color:red;'>{err}</div>"

def generate_gemini(query, model, system_prompt, prompt_type):
    try:
        load_dotenv()

        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable is not set.")

        import google.generativeai as genai
        genai.configure(api_key=api_key)

        model = genai.GenerativeModel(model_name=model)

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (å¼•æ•°ã‚’ä½¿ç”¨)

        if prompt_type == "Web App":
            last_message = f"Create a web application that: {query}"
        else:
            last_message = query
        response = model.generate_content(
            [
                {"role": "user", "parts": [{"text": system_prompt}]},
                {"role": "model", "parts": [{"text": "I understand and will follow these guidelines."}]},
                {"role": "user", "parts": [{"text": last_message}]}
            ],
            stream=False
        )

        response_text = response.text
        code = remove_code_block(response_text)
        preview = send_to_preview(code)
        return code, preview
    except Exception as e:
        err = f"Error in Gemini: {str(e)}"
        return err, f"<div style='padding: 8px;color:red;'>{err}</div>"

def generate_deepseek(query, model, system_prompt, prompt_type):
    try:
        # DeepSeek APIã‚­ãƒ¼ã®å–å¾—
        api_key = os.environ.get("DEEPSEEK_API_KEY")
        if not api_key:
            raise ValueError("DEEPSEEK_API_KEY environment variable is not set.")

        client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com/v1"
        )

        # DeepSeek API å‘¼ã³å‡ºã— (system_promptã‚’ä½¿ç”¨)
        if prompt_type == "Web App":
            user_msg = f"Create a web application that: {query}"
        else:
            user_msg = query
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_msg}
            ],
            temperature=0.7,
            max_tokens=2048,
            stream=False
        )

        response_text = response.choices[0].message.content
        code = remove_code_block(response_text)
        preview = send_to_preview(code)
        return code, preview
    except Exception as e:
        err = f"Error in DeepSeek: {str(e)}"
        return err, f"<div style='padding: 8px;color:red;'>{err}</div>"

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã®è¿½åŠ 
def remove_code_block(text):
    """ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹"""
    pattern = r'```(?:html)?\n(.+?)\n```'
    match = re.search(pattern, text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text.strip()

# send_to_previewé–¢æ•°ã‚’æ›´æ–°
def send_to_preview(code, iframe_id=""):
    """
    HTMLãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ã§ã™ã€‚
    ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã« <base> ã‚¿ã‚°ã‚’è¿½åŠ ã—ã€iframe å†…ã§ã®ç›¸å¯¾ URL ã®è§£æ±ºã‚’ä¿è¨¼ã—ã¾ã™ã€‚
    """
    clean_code = code.replace("```html", "").replace("```", "").strip()

    # æ—¢ã«HTMLæ–‡æ›¸ã§ãªã‘ã‚Œã°ã€<base>ã‚¿ã‚°ä»˜ãã®HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ãƒ©ãƒƒãƒ—
    if "<html" not in clean_code.lower():
        wrapped_code = f"""<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <base href="{BASE_URL}/">
  </head>
  <body>
    {clean_code}
  </body>
</html>"""
    else:
        # æ—¢å­˜ã®HTMLãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å ´åˆã¯<head>ã‚¿ã‚°å†…ã«<base>ã‚¿ã‚°ã‚’è¿½åŠ 
        if "<head>" in clean_code.lower():
            wrapped_code = clean_code.replace(
                "<head>",
                f'<head>\n    <base href="{BASE_URL}/">'
            )
        else:
            # <head>ã‚¿ã‚°ãŒãªã„å ´åˆã¯è¿½åŠ 
            wrapped_code = clean_code.replace(
                "<html>",
                f'<html>\n  <head>\n    <base href="{BASE_URL}/">\n  </head>'
            )

    encoded_html = base64.b64encode(wrapped_code.encode('utf-8')).decode('utf-8')
    data_uri = f"data:text/html;charset=utf-8;base64,{encoded_html}"

    id_attribute = f' id="{iframe_id}"' if iframe_id else ""
    return f'''
        <iframe{id_attribute}
            src="{data_uri}"
            style="width:100%;border:none;border-radius:4px;"
            sandbox="allow-scripts allow-same-origin"
        ></iframe>
    '''

# æ—¢å­˜ã®send_to_previewé–¢æ•°ã¯ãã®ã¾ã¾ã¨ã—ã¦ã€ä»¥ä¸‹ã«æ–°ã—ã„é–¢æ•° send_to_preview_react ã‚’è¿½åŠ 

def send_to_preview_react(react_code, container_id=""):
    """
    LLM ãŒç”Ÿæˆã—ãŸ React ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ã§ã™ã€‚

    â€» ã“ã®å®Ÿè£…ã¯è©¦ä½œç”¨ã§ã‚ã‚Šã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ã¯æœ€å°é™ã§ã™ã€‚

    ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã¯ã€Reactã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆä¾‹: GeneratedComponentï¼‰ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹å‰æã§ã™ã€‚
    Babel ã‚’åˆ©ç”¨ã—ã¦ JSX ã‚’ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã€ReactDOM ã§ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚
    """
    if not container_id:
        container_id = "react_preview"
    html_react = f"""
    <div id="{container_id}"></div>
    <!-- React ã¨ ReactDOM ã®èª­ã¿è¾¼ã¿ï¼ˆé–‹ç™ºç”¨ç‰ˆï¼‰ -->
    <script crossorigin src="https://unpkg.com/react@17/umd/react.development.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@17/umd/react-dom.development.js"></script>
    <!-- Babel ã®èª­ã¿è¾¼ã¿ï¼ˆJSXã‚’ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹ãŸã‚ï¼‰ -->
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <!-- ç”Ÿæˆã•ã‚ŒãŸ React ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€ã‚¹ã‚¯ãƒªãƒ—ãƒˆ -->
    <script type="text/babel">
    {react_code}
    // ä¾‹: LLM ã«ã‚ˆã‚Šç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰å†…ã§ GeneratedComponent ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã¨ä»®å®š
    ReactDOM.render(<GeneratedComponent />, document.getElementById("{container_id}"));
    </script>
    """
    return html_react

# æ—¢å­˜ã®importã«è¿½åŠ 
from concurrent.futures import ThreadPoolExecutor

# éåŒæœŸã®LLMç”Ÿæˆé–¢æ•°ã‚’æ›´æ–°
async def async_generate_openai(query, model, system_prompt, prompt_type):
    try:
        # åŒæœŸçš„ãª generate_openai ã‚’ asyncio.to_thread ã§å®Ÿè¡Œã—ã€ä¸¦åˆ—åŒ–
        return await asyncio.to_thread(generate_openai, query, model, system_prompt, prompt_type)
    except Exception as e:
        logger.error(f"Error in async OpenAI generation: {str(e)}")
        return (f"Error in OpenAI: {str(e)}",
                f"<div style='padding: 8px;color:red;'>Error in OpenAI: {str(e)}</div>")

async def async_generate_anthropic(query, model, system_prompt, prompt_type):
    try:
        # åŒæœŸçš„ãª generate_anthropic ã‚’ asyncio.to_thread ã§å®Ÿè¡Œã—ã€ä¸¦åˆ—åŒ–
        return await asyncio.to_thread(generate_anthropic, query, model, system_prompt, prompt_type)
    except Exception as e:
        logger.error(f"Error in async Anthropic generation: {str(e)}")
        return (f"Error in Anthropic: {str(e)}",
                f"<div style='padding: 8px;color:red;'>Error in Anthropic: {str(e)}</div>")

async def async_generate_gemini(query, model, system_prompt, prompt_type):
    try:
        # åŒæœŸçš„ãª generate_gemini ã‚’ asyncio.to_thread ã§å®Ÿè¡Œã—ã€ä¸¦åˆ—åŒ–
        return await asyncio.to_thread(generate_gemini, query, model, system_prompt, prompt_type)
    except Exception as e:
        logger.error(f"Error in async Gemini generation: {str(e)}")
        return (f"Error in Gemini: {str(e)}",
                f"<div style='padding: 8px;color:red;'>Error in Gemini: {str(e)}</div>")

async def async_generate_deepseek(query, model, system_prompt, prompt_type):
    try:
        # åŒæœŸçš„ãª generate_deepseek ã‚’ asyncio.to_thread ã§å®Ÿè¡Œã—ã€ä¸¦åˆ—åŒ–
        return await asyncio.to_thread(generate_deepseek, query, model, system_prompt, prompt_type)
    except Exception as e:
        logger.error(f"Error in async DeepSeek generation: {str(e)}")
        return (f"Error in DeepSeek: {str(e)}",
                f"<div style='padding: 8px;color:red;'>Error in DeepSeek: {str(e)}</div>")

# çµ±åˆç”Ÿæˆé–¢æ•°ã‚’ç°¡ç´ åŒ–
async def get_implementation_plan(query, prompt_type):
    """o3-miniã‚’ä½¿ç”¨ã—ã¦å®Ÿè£…è¨ˆç”»ã‚’ç”Ÿæˆã™ã‚‹"""
    try:
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set.")

        client = OpenAI(api_key=api_key)

        planning_prompt = """ã‚ãªãŸã¯å„ªç§€ãªã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®è¦ä»¶ã«å¯¾ã™ã‚‹å®Ÿè£…è¨ˆç”»ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

1. è¦ä»¶ã®åˆ†æ
2. å¿…è¦ãªæ©Ÿèƒ½ã®æ´—ã„å‡ºã—
3. å®Ÿè£…æ‰‹é †ã®è©³ç´°åŒ–
4. æ³¨æ„ç‚¹ã‚„ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

å›ç­”ã¯ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¡Œã£ã¦ãã ã•ã„ï¼š

<å®Ÿè£…è¨ˆç”»>
[ã“ã“ã«è¨ˆç”»ã®è©³ç´°ã‚’è¨˜è¼‰]
</å®Ÿè£…è¨ˆç”»>"""

        if prompt_type == "Web App":
            user_msg = f"ä»¥ä¸‹ã®Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè£…è¨ˆç”»ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š{query}"
        else:
            user_msg = f"ä»¥ä¸‹ã®æ©Ÿèƒ½ã®å®Ÿè£…è¨ˆç”»ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š{query}"

        response = client.chat.completions.create(
            model="o3-mini",
            messages=[
                {"role": "system", "content": planning_prompt},
                {"role": "user", "content": user_msg}
            ],
            stream=False
        )

        plan = response.choices[0].message.content
        return plan
    except Exception as e:
        logger.error(f"Error in implementation planning: {str(e)}")
        return f"Error in planning: {str(e)}"

async def generate_parallel(query, selected_models, system_prompt, prompt_type, use_planning=False):
    logger.info(f"Received generation request - Query: {query}")
    logger.info(f"Selected models: {selected_models}")

    implementation_plan = ""
    if use_planning:
        implementation_plan = await get_implementation_plan(query, prompt_type)
        # å®Ÿè£…è¨ˆç”»ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
        system_prompt = f"{system_prompt}\n\nå®Ÿè£…è¨ˆç”»ï¼š\n{implementation_plan}"

    # åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶å¾¡ã™ã‚‹ã‚»ãƒãƒ•ã‚©ã‚’ä½œæˆï¼ˆå¿…è¦ã«å¿œã˜ã¦æ•°å€¤ã‚’èª¿æ•´ï¼‰
    semaphore = asyncio.Semaphore(5)  # åŒæ™‚ã«5ã¤ã¾ã§å®Ÿè¡Œå¯èƒ½

    async def run_with_semaphore(full_model, task):
        async with semaphore:
            return await task

    tasks = []
    for full_model in selected_models:
        try:
            provider, model = full_model.split(":")
            logger.info(f"Preparing task for {full_model}")

            if provider == "openai":
                task = async_generate_openai(query, model, system_prompt, prompt_type)
            elif provider == "anthropic":
                task = async_generate_anthropic(query, model, system_prompt, prompt_type)
            elif provider == "gemini":
                task = async_generate_gemini(query, model, system_prompt, prompt_type)
            elif provider == "deepseek":
                task = async_generate_deepseek(query, model, system_prompt, prompt_type)
            else:
                logger.error(f"Unknown provider: {full_model}")
                continue

            tasks.append((full_model, run_with_semaphore(full_model, task)))

        except Exception as e:
            logger.error(f"Error preparing task for {full_model}: {str(e)}")
            continue

    results = []
    if tasks:
        completed_tasks = await asyncio.gather(*(task for _, task in tasks))
        for (full_model, _), result in zip(tasks, completed_tasks):
            # await ã§çµæœã‚’å–ã‚Šå‡ºã—ã¦ã‹ã‚‰ãƒªã‚¹ãƒˆã«è¿½åŠ 
            code, preview = result
            results.append((full_model, code, preview))

    # HTMLã®ç”Ÿæˆï¼ˆplan_htmlã‚’å‰Šé™¤ï¼‰
    grid_html = """
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-coy.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-markup.min.js"></script>
    <div class='results-container'>
        <div class='results-grid'>
    """

    # çµæœã‚«ãƒ¼ãƒ‰ã®ç”Ÿæˆ
    for full_model, code, preview in results:
        provider, model_name = full_model.split(":")
        model_id = f"model_{provider}_{model_name}".replace("-", "_")

        preview_iframe = send_to_preview(code, iframe_id=f"{model_id}_preview")
        escaped_code = code.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

        grid_html += f"""
            <div class='result-card'>
                <div class='card-header'>
                    <div class='header-title'>
                        <strong>{provider.upper()}</strong> - {model_name}
                    </div>
                    <div class='header-buttons'>
                        <button class="button-icon" onclick="(function(){{
                            var codeEl = document.getElementById('{model_id}_code');
                            if (codeEl){{
                                codeEl.style.display = (codeEl.style.display === 'none' ? 'block' : 'none');
                                if (codeEl.style.display === 'block' && window.Prism){{ Prism.highlightAll(); }}
                            }}
                        }})()" title="ã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¤º/éè¡¨ç¤º">
                            <svg viewBox="0 0 24 24">
                                <path fill="currentColor" d="M9.4 16.6L4.8 12l4.6-4.6L8 6l-6 6 6 6 1.4-1.4zm5.2 0l4.6-4.6-4.6-4.6L16 6l6 6-6 6-1.4-1.4z"/>
                            </svg>
                        </button>
                        <button class="button-icon" onclick="(function(){{ 
                            var iframe = document.getElementById('{model_id}_preview');
                            if (iframe){{
                                var currentSrc = iframe.src;
                                iframe.src = 'about:blank';
                                setTimeout(function() {{
                                    iframe.src = currentSrc;
                                }}, 100);
                            }}
                        }})()" title="ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ›´æ–°">
                            <svg viewBox="0 0 24 24">
                                <path fill="currentColor" d="M17.65 6.35A7.958 7.958 0 0012 4c-4.42 0-7.99 3.58-7.99 8s3.57 8 7.99 8c3.73 0 6.84-2.55 7.73-6h-2.08A5.99 5.99 0 0112 18c-3.31 0-6-2.69-6-6s2.69-6 6-6c1.66 0 3.14.69 4.22 1.78L13 11h7V4l-2.35 2.35z"/>
                            </svg>
                        </button>
                    </div>
                </div>
                <div style='position: relative;'>
                    <div class='preview-container'>
                        {preview_iframe}
                    </div>
                    <div id='{model_id}_code' class='code-content' style='display:none;'>
                        <pre><code class="language-html">{escaped_code}</code></pre>
                    </div>
                </div>
            </div>
        """

    grid_html += """
        </div>
    </div>
    """

    logger.info("Completed generating HTML grid")
    return grid_html

# Kroki.ioã‚’ä½¿ã£ã¦SVGã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_kroki_svg(diagram_source, diagram_type):
    """
    Kroki.ioã‚’ä½¿ã£ã¦å›³ã®SVGã‚’å–å¾—ã™ã‚‹
    
    Args:
        diagram_source (str): å›³ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
        diagram_type (str): å›³ã®ã‚¿ã‚¤ãƒ— (excalidraw, graphviz, mermaid)
        
    Returns:
        str: SVGå½¢å¼ã®å›³ã€ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    try:
        # POSTãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½¿ç”¨ã™ã‚‹æ–¹æ³•
        url = f"https://kroki.io/{diagram_type}/svg"
        headers = {
            "Content-Type": "text/plain"
        }
        response = requests.post(url, headers=headers, data=diagram_source)
        
        if response.status_code == 200:
            return response.text
        else:
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
            return f"<div class='error'>Error: {response.status_code} - {response.text}</div>"
    except Exception as e:
        logger.error(f"Error getting SVG from Kroki.io: {str(e)}")
        return f"<div class='error'>Error: {str(e)}</div>"

# å›³ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°
def send_to_diagram_preview(diagram_source, diagram_type):
    """
    å›³ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰SVGã‚’å–å¾—ã—ã¦ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹
    
    Args:
        diagram_source (str): å›³ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
        diagram_type (str): å›³ã®ã‚¿ã‚¤ãƒ— (excalidraw, graphviz, mermaid)
        
    Returns:
        str: HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    """
    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰å›³ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    pattern = r"```(?:" + diagram_type + r")?\s*([\s\S]*?)\s*```"
    match = re.search(pattern, diagram_source, re.IGNORECASE)
    
    if match:
        source_code = match.group(1).strip()
        svg_content = get_kroki_svg(source_code, diagram_type)
        
        html_content = f"""
        <div class="result-card">
            <div class="card-header" style="display: flex; justify-content: space-between; padding: 8px 16px; align-items: center;">
                <div class="header-title">
                    <strong>{diagram_type.capitalize()} Diagram</strong>
                </div>
            </div>
            <div class="diagram-preview" style="padding: 16px; overflow: auto;">
                {svg_content}
            </div>
            <div class="code-content">
                <pre><code>{source_code}</code></pre>
            </div>
        </div>
        """
        return html_content
    else:
        return f"<div class='error'>Error: No {diagram_type} code block found in the response.</div>"

# çµ±åˆGradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®å®šç¾©
def build_interface():
    custom_css = """
    :root {
        --card-bg: #ffffff;
        --header-bg: #f5f5f5;
        --border-color: #e0e0e0;
        --text-color: #333333;
        --icon-color: #333333;
        --button-hover-bg: rgba(0, 0, 0, 0.1);
        --code-bg: #f5f5f5;
        --code-text: #333333;
        --preview-bg: #ffffff;
        --preview-border: #e0e0e0;
    }
    * {
        font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
    }

    /* çµæœå‡ºåŠ›ã‚¨ãƒªã‚¢ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .result-output {
        min-height: 600px !important;
        margin-bottom: 2rem;
    }

    /* ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ã‚³ãƒ³ãƒ†ãƒŠ */
    .progress-container {
        background: var(--neutral-100);
        padding: 1rem;
        border-radius: 8px;
        min-height: 100px;
        display: flex;
        align-items: center;
        justify-content: center;
    }

    /* ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .progress-bar {
        height: 4px;
        background: var(--primary-500);
        border-radius: 2px;
        animation: progress 2s infinite;
    }

    @keyframes progress {
        0% { width: 0%; }
        50% { width: 70%; }
        100% { width: 100%; }
    }

    /* å®Ÿè£…è¨ˆç”»ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .implementation-plan {
        margin: 20px 0;
        padding: 20px;
        border-radius: 8px;
        background: var(--neutral-50);
    }

    /* çµæœã‚«ãƒ¼ãƒ‰ã®ã‚¹ã‚¿ã‚¤ãƒ«æ›´æ–° */
    .result-card {
        background: var(--card-bg);
        border: 1px solid var(--border-color);
        border-radius: 8px;
        overflow: hidden;
        margin-bottom: 32px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        color: var(--text-color);
    }

    /* ã‚«ãƒ¼ãƒ‰ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ«ã®ä¿®æ­£ */
    .card-header {
        background: var(--header-bg, #f5f5f5);
        border-bottom: 1px solid var(--border-color, #e0e0e0);
        color: var(--text-color, #333333);
    }

    /* ãƒ˜ãƒƒãƒ€ãƒ¼å†…ã®å¼·èª¿ãƒ†ã‚­ã‚¹ãƒˆã®ä¿®æ­£ */
    .header-title strong {
        color: var(--text-color, #333333);
        margin-right: 4px;
    }

    /* ãƒ˜ãƒƒãƒ€ãƒ¼ãƒœã‚¿ãƒ³ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .header-buttons {
        display: flex;
        gap: 8px;
    }

    .button-icon {
        background: none;
        border: 1px solid var(--border-color, #404040);
        cursor: pointer;
        padding: 4px;
        border-radius: 4px;
        display: flex;
        align-items: center;
        justify-content: center;
        width: 32px;
        height: 32px;
        transition: all 0.2s;
        color: var(--icon-color, #333333);
    }

    .button-icon:hover {
        background-color: var(--button-hover-bg, rgba(0, 0, 0, 0.1));
        border-color: var(--icon-color, #333333);
    }

    .button-icon svg {
        width: 20px;
        height: 20px;
        color: var(--icon-color, #333333);
    }

    .preview-container {
        width: 100%;
        min-height: 600px !important;  /* æœ€å°é«˜ã•ã‚’è¨­å®š */
        height: 800px;  /* ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é«˜ã•ã‚’è¨­å®š */
        position: relative;
        background: var(--preview-bg);
        border: 1px solid var(--preview-border);
        border-radius: 4px;
        margin: 16px;
        overflow: auto;  /* ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ã« */
    }

    .preview-container iframe {
        width: 100%;
        height: 100%;
        min-height: 600px !important;  /* iframeã®æœ€å°é«˜ã•ã‚‚è¨­å®š */
        border: none;
        background: var(--preview-bg);
    }

    /* ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .code-content {
        background: var(--code-bg, #1e1e1e);
        color: var(--code-text, #d4d4d4);
        padding: 16px;
        margin: 16px;
        border-radius: 4px;
        overflow-x: auto;
        max-height: 800px;
        overflow-y: auto;
        border: 1px solid var(--border-color);
    }

    .code-content pre {
        margin: 0;
        padding: 0;
        background: transparent;
    }

    .code-content code {
        font-family: 'Consolas', 'Monaco', 'Andale Mono', monospace;
        font-size: 14px;
        line-height: 1.5;
        color: inherit;
    }

    /* å›³ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã‚¹ã‚¿ã‚¤ãƒ« */
    .diagram-preview {
        padding: 16px;
        background: white;
        border-radius: 4px;
        overflow: auto;
        text-align: center;
    }

    .diagram-preview svg {
        max-width: 100%;
        height: auto;
    }

    .error {
        color: red;
        padding: 16px;
        background: #ffeeee;
        border-radius: 4px;
        margin: 16px;
    }
    """
    with gr.Blocks(
        css=custom_css,
        theme=gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="slate",
            neutral_hue="slate",
            font=["system-ui", "-apple-system", "sans-serif"]
        ).set(
            background_fill_primary="#ffffff",
            background_fill_secondary="#f5f5f5",
            border_color_primary="#e0e0e0"
        )
    ) as demo:
        gr.Markdown("# ğŸ¨ AI Gradio Code Generator")

        # å…¥åŠ›ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        gr.Markdown("## å…¥åŠ›")
        with gr.Row():
            # å·¦å´ã®ã‚«ãƒ©ãƒ 
            with gr.Column(scale=1):
                # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚¨ãƒªã‚¢
                query_input = gr.Textbox(
                    placeholder="ä½œæˆã—ãŸã„Webã‚¢ãƒ—ãƒªã‚„å›³ã®ä»•æ§˜ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„",
                    label="Request",
                    lines=8
                )
                # ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆ: çµ±åˆå¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
                model_select = gr.Dropdown(
                    choices=INTEGRATED_MODELS,
                    value=[
                       # INTEGRATED_MODELS[4],
                        # INTEGRATED_MODELS[5],
                        # INTEGRATED_MODELS[6],
                        INTEGRATED_MODELS[7],
                        # INTEGRATED_MODELS[8],
                    ],
                    multiselect=True,
                    label="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
                    info="è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã¾ã™"
                )

                # å®Ÿè£…è¨ˆç”»ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ã“ã“ã«ç§»å‹•
                use_planning = gr.Radio(
                    choices=["ã¯ã„", "ã„ã„ãˆ"],
                    label="o3-miniã«ã‚ˆã‚‹å®Ÿè£…è¨ˆç”»ã‚’åˆ©ç”¨ã—ã¾ã™ã‹ï¼Ÿ",
                    value="ã„ã„ãˆ",
                    info="o3-miniãŒå®Ÿè£…è¨ˆç”»ã‚’ä½œæˆã—ã€ãã®è¨ˆç”»ã«åŸºã¥ã„ã¦å„ãƒ¢ãƒ‡ãƒ«ãŒå®Ÿè£…ã‚’è¡Œã„ã¾ã™ã€‚"
                )

            # å³å´ã®ã‚«ãƒ©ãƒ 
            with gr.Column(scale=1):
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³
                prompt_type = gr.Radio(
                    ["Web App", "Text", "Excalidraw", "GraphViz", "Mermaid"],
                    label="Prompt Type",
                    value="Web App",
                    interactive=True
                )

                # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›æ¬„ (Web App ç”¨, Textç”¨, Excalidrawç”¨, GraphVizç”¨, Mermaidç”¨)
                system_prompt_webapp_textbox = gr.Textbox(
                    placeholder="Enter system prompt for web app generation...",
                    label="System Prompt (Web App)",
                    lines=5,
                    value=DEFAULT_WEBAPP_SYSTEM_PROMPT,
                    visible=True
                )
                system_prompt_text_textbox = gr.Textbox(
                    placeholder="Enter system prompt for text generation...",
                    label="System Prompt (Text)",
                    lines=5,
                    value=DEFAULT_TEXT_SYSTEM_PROMPT,
                    visible=False
                )
                system_prompt_excalidraw_textbox = gr.Textbox(
                    placeholder="Enter system prompt for Excalidraw diagram generation...",
                    label="System Prompt (Excalidraw)",
                    lines=5,
                    value=DEFAULT_EXCALIDRAW_SYSTEM_PROMPT,
                    visible=False
                )
                system_prompt_graphviz_textbox = gr.Textbox(
                    placeholder="Enter system prompt for GraphViz diagram generation...",
                    label="System Prompt (GraphViz)",
                    lines=5,
                    value=DEFAULT_GRAPHVIZ_SYSTEM_PROMPT,
                    visible=False
                )
                system_prompt_mermaid_textbox = gr.Textbox(
                    placeholder="Enter system prompt for Mermaid diagram generation...",
                    label="System Prompt (Mermaid)",
                    lines=5,
                    value=DEFAULT_MERMAID_SYSTEM_PROMPT,
                    visible=False
                )

        # å®Ÿè£…è¨ˆç”»ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆçµæœã®å‰ã«é…ç½®ï¼‰
        plan_output = gr.Markdown(
            visible=False,
            elem_classes="implementation-plan"
        )

        # Generate ãƒœã‚¿ãƒ³
        generate_btn = gr.Button(
            "Generate",
            variant="primary",
            size="lg"
        )

        # çµæœã‚»ã‚¯ã‚·ãƒ§ãƒ³
        gr.Markdown("## çµæœ")
        output_html = gr.HTML(
            container=True,
            show_label=True,
            elem_classes="result-output"
        )

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ system_prompt ã‚’æ±ºå®šã™ã‚‹é–¢æ•°
        def get_system_prompt(prompt_type, webapp_prompt, text_prompt, excalidraw_prompt, graphviz_prompt, mermaid_prompt):
            if prompt_type == "Web App":
                return webapp_prompt
            elif prompt_type == "Text":
                return text_prompt
            elif prompt_type == "Excalidraw":
                return excalidraw_prompt
            elif prompt_type == "GraphViz":
                return graphviz_prompt
            elif prompt_type == "Mermaid":
                return mermaid_prompt
            else:
                return webapp_prompt  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—ã®å¤‰æ›´ã«å¿œã˜ã¦ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›æ¬„ã®è¡¨ç¤º/éè¡¨ç¤ºã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹
        def update_system_prompt_visibility(prompt_type):
            return {
                system_prompt_webapp_textbox: prompt_type == "Web App",
                system_prompt_text_textbox: prompt_type == "Text",
                system_prompt_excalidraw_textbox: prompt_type == "Excalidraw",
                system_prompt_graphviz_textbox: prompt_type == "GraphViz",
                system_prompt_mermaid_textbox: prompt_type == "Mermaid"
            }

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—ã®å¤‰æ›´ã‚¤ãƒ™ãƒ³ãƒˆã‚’è¨­å®š
        prompt_type.change(
            fn=update_system_prompt_visibility,
            inputs=[prompt_type],
            outputs=[
                system_prompt_webapp_textbox,
                system_prompt_text_textbox,
                system_prompt_excalidraw_textbox,
                system_prompt_graphviz_textbox,
                system_prompt_mermaid_textbox
            ]
        )

        # ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯æ™‚ã®å‡¦ç†ã‚’æ›´æ–°
        async def run_generate(q, m, pt, wp, tp, ep, gp, mp, up):
            try:
                # éãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ã§ã®ãƒ­ãƒƒã‚¯å–å¾—ã‚’è©¦ã¿ã‚‹ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’çŸ­ãè¨­å®šï¼‰
                await asyncio.wait_for(generation_lock.acquire(), timeout=0.001)
            except asyncio.TimeoutError:
                logger.info("Generation is already in progress. Ignoring duplicate request.")
                return [plan_output, "<div style='padding: 8px;color:red;'>Process already in progress. Please wait...</div>"]
            try:
                use_plan = (up == "ã¯ã„")
                if use_plan:
                    implementation_plan = await get_implementation_plan(q, pt)
                    logger.info("Implementation Plan:")
                    logger.info(implementation_plan)
                    plan_output.visible = True
                    plan_output.value = f"## å®Ÿè£…è¨ˆç”» (o3-mini)\n\n{implementation_plan}"
                else:
                    plan_output.visible = False
                    implementation_plan = ""

                result = await generate_parallel(
                    q, m,
                    get_system_prompt(pt, wp, tp, ep, gp, mp),
                    pt,
                    use_planning=use_plan
                )

                # å›³ã®å ´åˆã¯Kroki.ioã‚’ä½¿ã£ã¦ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤º
                if pt in ["Excalidraw", "GraphViz", "Mermaid"]:
                    diagram_type = pt.lower()
                    diagram_preview = send_to_diagram_preview(result, diagram_type)
                    return [plan_output, diagram_preview]
                
                return [plan_output, result]
            finally:
                generation_lock.release()


        generate_btn.click(
            fn=run_generate,
            inputs=[
                query_input,
                model_select,
                prompt_type,
                system_prompt_webapp_textbox,
                system_prompt_text_textbox,
                system_prompt_excalidraw_textbox,
                system_prompt_graphviz_textbox,
                system_prompt_mermaid_textbox,
                use_planning
            ],
            outputs=[plan_output, output_html]
        )
    return demo

if __name__ == "__main__":
    demo = build_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        show_error=True
    ) 
